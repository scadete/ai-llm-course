{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bTt0jC-0OQW"
      },
      "source": [
        "# Memória\n",
        "\n",
        "Os LLM são essencialmente stateless, isto é não guardam histórico e cada interação é uma nova chamada na API. Cada transação é independente.\n",
        "Chatbots aparentam ter memória porque o código que o implementa considera o histórico de toda conversa no contexto.\n",
        "\n",
        "Langchain oferece diferentes tipos de gerenciadores de memória.\n",
        "Abordaremos desses:\n",
        "- ConversationBufferMemory\n",
        "- ConversationBufferWindowMemory\n",
        "- ConversationTokenBufferMemory\n",
        "- ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMy4WmXq1grR"
      },
      "source": [
        "## Preparando o Ambiente\n",
        "\n",
        "Para iniciar um trabalho sobre a API da OpenAI, vamos primeiro importar a biblioteca python da openai. Para facilitar o carregamento seguro da chave de API utilizada, também utilizaremos a biblioteca python-dotenv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install openai\n",
        "%pip install --upgrade langchain\n",
        "%pip install langchain-community\n",
        "%pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fLf9EebGJMO",
        "outputId": "46186765-5e9c-49a7-fd55-f5c9fde9e440"
      },
      "outputs": [],
      "source": [
        "# Setup environment and import required libraries\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%set_env OPENAI_API_KEY=#Your API Key here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_model=\"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Histórico de Navegação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HadZVTOqubkd"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To control the randomness and creativity of the generated\n",
        "# text by an LLM, use temperature = 0.0\n",
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory = memory,\n",
        "    verbose=True # Esse comando permite ver o que a LLM está aplicando\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Oi, me chamo Saulo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Qual a resposta para a pergunta sobre o universo e tudo mais?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation.predict(input=\"qual meu nome?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A variável memory armazena a memória da conversa até então. Vamos ver seu valor a seguir:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se quisermos alterar um conteúdo, dizendo qual o contexto e memória pregresso da uma interação, pode ser definida uma variável nova para isso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com uma memória vazia, adicionar os elementos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Oi\"},\n",
        "    {\"output\": \"Opa, novidades?\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Nada demais\"},\n",
        "    {\"output\": \"Massa\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Janela de Memória\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Oi\"},\n",
        "    {\"output\": \"Opa, novidades?\"}\n",
        ")\n",
        "\n",
        "memory.save_context(\n",
        "    {\"input\": \"Nada demais\"},\n",
        "    {\"output\": \"Massa\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Oi, meu nome é Saulo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Qual capital de Pernambuco?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Qual meu nome?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Token\n",
        "\n",
        "A memória também pode ser controlada usando a quantidade de tokens como referência."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
        "memory.save_context({\"input\": \"IA é o que?!\"},\n",
        "                    {\"output\": \"Arretada!\"})\n",
        "memory.save_context({\"input\": \"Aprendizagem de máquina é o que?\"},\n",
        "                    {\"output\": \"Demais!\"})\n",
        "memory.save_context({\"input\": \"Chatbots são o que?\"}, \n",
        "                    {\"output\": \"Simpáticos!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Resumo\n",
        "\n",
        "Resumo para combinar de forma eficiente o conteúdo passado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Oi\"}, {\"output\": \"Conta as novas\"})\n",
        "memory.save_context({\"input\": \"Nada demais, só passando tempo\"},\n",
        "                    {\"output\": \"Legal\"})\n",
        "memory.save_context({\"input\": \"Qual a agenda de hoje?\"}, \n",
        "                    {\"output\": f\"{schedule}\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
